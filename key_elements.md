**Learning Objectives**: 

*LO7a: To understand the history of peer review, and place current developments in Open Peer Review in that context (knowledge).*

*LO7b: To gain insight into the process of responsible research evaluation, and the role that peer review and traditional and next-generation metrics play in this (knowledge).*

*LO7c: To be able to identify and apply a range of metrics to demonstrate the broader impact of your research outputs (tasks).*

### Key components:

* Fundamentals of good peer review.

* History of peer review and scholarly publishing.

* Types of open peer review and new models.

* Pros and cons associated with different types of open peer review, including post-publication peer review, commenting and annotation.

* Issues with traditional methods of research assessment and evaluation.

* The San Francisco Declaration on Research Assessment ([DORA](http://www.ascb.org/dora/)), Leiden Manifesto, and Metric Tide reports.

* Next generation metrics (aka altmetrics), responsible metrics use and peer review.

* Role of metrics in research evaluation, funding, promotion, signalling and reporting.

* Differentiating between impact and attention.

### Who to involve:

* Individuals: Nikolaus Kriegeskorte, Irene Hames, Tony Ross-Hellauer, Peter Kraker, Michael Markie, Sabina Alam, Elizabeth Gadd, William Gunn.

* Organisations: OpenAIRE, ScienceOpen, [Publons](https://publons.com/home/), [PubPeer](https://pubpeer.com/), [OpenUP](http://openup-h2020.eu/contact/), Altmetric, ImpactStory, BioMed Central, Frontiers, eLife, [PEERE](http://www.peere.org/).

* Other: Editorial staff at journals offering traditional peer review.

### Key resources:

**Tools**

* [Peer review report template](https://www.authorea.com/templates/peer_review_report_template), Authorea.

* [Eigenfactor](http://www.eigenfactor.org/index.php) project.

* [Publons Academy](https://publons.com/community/academy).

* [Metrics Toolkit](http://www.metrics-toolkit.org/).

* [Open Badges](https://openbadges.org/).

* [Open Review Toolkit](https://www.bitbybitbook.com/en/open-review/).

**Research Articles and Reports**

* [Why the impact factor of journals should not be used for evaluating research](http://europepmc.org/backend/ptpmcrender.fcgi?accid=PMC2126010&blobtype=pdf) (Seglen, 1997).

* [Effect of open peer review on quality of reviews and on reviewers' recommendations: a randomised trial](http://www.bmj.com/content/318/7175/23) (van Rooyen et al., 1999).

* [A Reliability-Generalization Study of Journal Peer Reviews: A Multilevel Meta-Analysis of Inter-Rater Reliability and Its Determinants](http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0014331) (Bornmann et al., 2010).

* [Effect on peer review of telling reviewers that their signed reviews might be posted on the web: randomised controlled trial](http://www.bmj.com/content/341/bmj.c5729) (van Rooyen et al., 2010).

* [Open peer review: A randomised controlled trial](https://www.cambridge.org/core/journals/the-british-journal-of-psychiatry/article/open-peer-review-a-randomised-controlled-trial/1F81447FC67B3BAFDCCCCE82B6C7A187) (Walsh et al., 2010).

* [Deep impact: unintended consequences of journal rank](https://www.frontiersin.org/articles/10.3389/fnhum.2013.00291/full) (Brembs et al., 2013).

* [Excellence by Nonsense: The Competition for Publications in Modern Science](https://link.springer.com/chapter/10.1007/978-3-319-00026-8_3) (Binswanger, 2014).

* [Attention! A study of open access vs non-open access articles](https://figshare.com/articles/Attention_A_study_of_open_access_vs_non_open_access_articles/1213690) (Adie, 2014).

* [Publishing: Credit where credit is due](http://www.nature.com/news/publishing-credit-where-credit-is-due-1.15033) (Allen et al., 2014).

* [The Metric Tide](https://responsiblemetrics.org/the-metric-tide/) report (Wilsdon et al., 2015).

* [Grand challenges in altmetrics: heterogeneity, data quality and dependencies](https://arxiv.org/abs/1603.04939) (Haustein, 2016).

* [Badges to Acknowledge Open Practices: A Simple, Low-Cost, Effective Method for Increasing Transparency](http://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.1002456) (Kidwell et al., 2016).

* [A framework to monitor open science trends in the EU](https://www.oecd.org/sti/063%20-%20OECD%20Blue%20Sky%202016_Open%20Science.pdf) (Smith et al., 2016).

* [Peer Review Survey 2015: Key Findings](http://publishingresearchconsortium.com/index.php/134-news-main-menu/prc-peer-review-survey-2015-key-findings/172-peer-review-survey-2015-key-findings) (Mark Ware Consulting, 2016).

* [Point of View: How open science helps researchers succeed](https://elifesciences.org/articles/16800) (McKiernan et al., 2016).

* [Peer Review Quality and Transparency of the Peer-Review Process in Open Access and Subscription Journals](http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0147913) (Wicherts, 2016).

* [Next-generation metrics: Responsible metrics and evaluation for open science](https://ec.europa.eu/research/openscience/pdf/report.pdf) (European Commission, 2017).

* [Evaluation of Research Careers fully acknowledging Open Science Practices: Rewards, incentives and/or recognition for researchers practicing Open Science](https://ec.europa.eu/research/openscience/pdf/os_rewards_wgreport_final.pdf) (European Commission, 2017).

* [Research: Gender bias in scholarly peer review](https://elifesciences.org/articles/21718) (Helmer et al., 2017).

* ["Excellence R Us": university research and the fetishisation of excellence](https://www.nature.com/articles/palcomms2016105) (Moore et al., 2017).

* [Metrics for openness](https://researchcommons.waikato.ac.nz/handle/10289/10842) (Nichols and Twidale, 2017).

* [What is open peer review? A systematic review](https://f1000research.com/articles/6-588/v2) (Ross-Hellauer, 2017).

* [Survey on open peer review: Attitudes and experience amongst editors, authors and reviewers](http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0189311) (Ross-Hellauer et al., 2017).

* [A multi-disciplinary perspective on emergent and future innovations in peer review](https://f1000research.com/articles/6-1151/v3) (Tennant et al., 2017).

* [Reviewer bias in single- versus double-blind peer review](http://www.pnas.org/content/114/48/12708) (Tomkins et al., 2017).

* [Prestigious science journals struggle to reach even average reliability](https://www.frontiersin.org/articles/10.3389/fnhum.2018.00037/full) (Brembs, 2018).

* [Making research evaluation more transparent: Aligning research philosophy, institutional values, and reporting](https://psyarxiv.com/48qux/) (Dougherty et al., 2018).

* [Research excellence indicators: time to reimagine the 'making of'?](https://academic.oup.com/spp/advance-article/doi/10.1093/scipol/scy007/4858431) (Ferretti et al., 2018).

* [The Journal Impact Factor: A brief history, critique, and discussion of adverse effects](https://arxiv.org/abs/1801.08992) (Lariviere and Sugimoto, 2018).

* [Scholarly Communication Librarians' Relationship with Research Impact Indicators: An Analysis of a National Survey of Academic Librarians in the United States](https://jlsc-pub.org/articles/abstract/10.7710/2162-3309.2212/) (Miles et al., 2018).

* [Assessing scientists for hiring, promotion, and tenure](http://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.2004089) (Moher et al., 2018).

* [Ten considerations for open peer review](https://f1000research.com/articles/7-969/v1) (Schmidt et al., 2018).

**Key posts**

* [Six essential reads on peer review](http://asapbio.org/six-essential-reads-on-peer-review), ASAPbio.

* [Peer reviews are open for registering at Crossref](https://www.crossref.org/blog/peer-reviews-are-open-for-registering-at-crossref/), Jennifer Lin.

* [Why we don't sign our peer reviews](http://www.molecularecologist.com/2014/04/why-we-dont-sign/), Jeremy Yoder.

* [The Fractured Logic of Blinded Peer Review in Journals](http://blogs.plos.org/absolutely-maybe/2017/10/31/the-fractured-logic-of-blinded-peer-review-in-journals/), Hilda Bastian.

* [The peer review process: challenges and progress](https://www.editage.com/insights/the-peer-review-process-challenges-and-progress), Irene Hames.

* [Responsible metrics: Where it's at?](https://thebibliomagician.wordpress.com/2018/02/16/responsible-metrics-where-its-at/), Lizzie Gadd.

* [Goodhart's Law and why measurement is hard](https://www.ribbonfarm.com/2016/06/09/goodharts-law-and-why-measurement-is-hard/), David Manheim.

* [Academe's prestige problem: We're all complicit in perpetuating a rigged system](https://www.chronicle.com/article/Academe-s-Prestige-Problem/241432), Maximillian Alvarez.

* [Let's move beyond the rhetoric: it's time to change how we judge research](https://www.nature.com/articles/d41586-018-01642-w), Stephen Curry.

* [Blockchain offers a true route to a scholarly commons](https://www.researchresearch.com/news/article/?articleId=1373351), Lambert Heller.

* [There is an absence of scientific authority over research assessment as a professional practice, leaving a gap that has been filled by database providers](http://blogs.lse.ac.uk/impactofsocialsciences/2018/07/25/there-is-an-absence-of-scientific-authority-over-research-assessment-as-a-professional-practice-leaving-a-gap-that-has-been-filled-by-database-providers/), Arlette Jappe, David Pithan and Thomas Heinze, LSE Impact Blog.

**Other**

* [Metrics and Research Assessment](https://www.scienceopen.com/search#collection/78c15291-27e3-493a-99ec-7e5a00387745), ScienceOpen collection.

* [The Open Access Citation Advantage](https://www.scienceopen.com/search#collection/996823e0-8104-4490-b26a-f2f733f810fb), ScienceOpen collection.

* [Citation Behaviour and Practice](https://www.scienceopen.com/search#collection/2d601af5-aa90-4c63-9a11-85f2dc768868), ScienceOpen collection.

* [Scholarly Publication Practices and Impact Factor Calculation and Manipulation](https://www.scienceopen.com/search#collection/e4870106-eea5-4ba3-88cf-e769c7d49ebe), ScienceOpen collection.

* [Peer Review in the Age of Open Science](https://www.slideshare.net/OpenAIRE_eu/peer-review-in-the-age-of-open-science), Tony Ross-Hellauer, 2017.

* [The San Francisco Declaration on Research Assessment](http://www.ascb.org/dora/) (DORA) and [Leiden Manifesto](http://www.leidenmanifesto.org/).

* The [Humane Metrics Initiative](http://humetricshss.org/about/).

* [Open Research Badges](https://openresearchbadges.org/).

* [OpenUpHub must reads](https://www.openuphub.eu/review/must-reads).

* [NISO Alternative Assessment Metrics (Altmetrics) Initiative](http://www.niso.org/standards-committees/altmetrics).

* [Snowball Metrics](https://www.snowballmetrics.com/), standardized research metrics.

### Tasks:

* Perform one open peer review on a paper of your choice at ScienceOpen, and get a DOI for it.

* Integrate one peer review (pre- or post-publication) experience into Publons.

* Use Publons journal list to check open peer review policies of journal(s) in your discipline.

* Sign DORA in either a personal or business-level capacity.

* Define your impact.

    * Write a personal impact statement about your research (actual or predicted). Avoid using journal titles or the journal impact factor.

    * Discover the Altmetric scores for your published items using their [bookmarklet](https://www.altmetric.com/products/free-tools/bookmarklet/).

* Track your research impact by integrating your ORCID profile with either ScienceOpen or ImpactStory (or both).

* Do you have a personal website? If not, now is a good time to design one and make all of the above information part of your digital profile.

* Find out what your research department or institutes research evaluation criteria are. Have a discussion about them with your research colleagues.

    * Find out who wrote them, and ask them what evidence they used to support the criteria.
